"use strict";(self.webpackChunkcookbook=self.webpackChunkcookbook||[]).push([[561],{24680:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"about","metadata":{"permalink":"/einstein-platform/about","source":"@site/cookbook/about.md","title":"About the Cookbook \ud83d\udcd6","description":"The Einstein Platform Cookbook is where Salesforce shares example code and recipes for building with the Einstein Platform. This website is generated based on the contents of the open source einstein-platform repository on GitHub.","date":"2024-09-09T00:00:00.000Z","tags":[{"inline":false,"label":"welcome","permalink":"/einstein-platform/tags/welcome"}],"readingTime":0.675,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"about","title":"About the Cookbook \ud83d\udcd6","tags":["welcome"],"date":"2024-09-09T00:00:00.000Z"},"unlisted":false,"nextItem":{"title":"Introducing the LLM Open Connector","permalink":"/einstein-platform/open-connector"}},"content":"The [Einstein Platform Cookbook](https://opensource.salesforce.com/einstein-platform/) is where Salesforce shares example code and recipes for building with the Einstein Platform. This website is generated based on the contents of the open source [einstein-platform](https://github.com/salesforce/einstein-platform) repository on GitHub.\\n\\nOur first batch of recipes have arrived! These recipes include step-by-step instructions for implementing the LLM Open Connector ([readme](https://github.com/salesforce/einstein-platform?tab=readme-ov-file#llm-open-connector), [spec](/docs/apis/llm-open-connector)) with an API gateway, and each recipe features a different AI platform. More recipes are in the works featuring new AI platforms and system architectures!\\n\\nYour contributions to the Cookbook and the open-source repository are welcome! Refer to the [Contributing Guide](https://github.com/salesforce/einstein-platform/blob/main/CONTRIBUTING.md) and [Code of Conduct](https://github.com/salesforce/einstein-platform/blob/main/CODE_OF_CONDUCT.md) to get started.\\n\\nIf you like the resources that you see here, consider adding a \u2b50 on GitHub. It helps other people discover them!\\n\\nSee Also:\\n\\n- [Generative AI Developer Guide](https://developer.salesforce.com/docs/einstein/genai/overview)\\n- [Einstein Platform GitHub Repo](https://github.com/salesforce/einstein-platform)"},{"id":"open-connector","metadata":{"permalink":"/einstein-platform/open-connector","source":"@site/cookbook/open-connector.md","title":"Introducing the LLM Open Connector","description":"The LLM Open Connector is an API specification, closely based on the OpenAI API, that lets you create API gateways and proxy servers that connect any language model to the Einstein AI Platform. Any service that implements the API specification can be connected to Einstein Studio using the Bring Your Own LLM (BYOLLM) feature.","date":"2024-09-10T00:00:00.000Z","tags":[{"inline":false,"label":"llm-open-connector","permalink":"/einstein-platform/tags/llm-open-connector"}],"readingTime":0.45,"hasTruncateMarker":false,"authors":[{"name":"Darvish","title":"Director, Product Management @ Salesforce","url":"https://github.com/dshadravan","page":{"permalink":"/einstein-platform/authors/dshadravan"},"socials":{"github":"https://github.com/dshadravan"},"imageURL":"https://github.com/dshadravan.png","key":"dshadravan"},{"name":"Oleksandr","title":"Software Engineer @ Salesforce","url":"https://github.com/oleksm","page":{"permalink":"/einstein-platform/authors/oleksm"},"socials":{"github":"https://github.com/oleksm"},"imageURL":"https://github.com/oleksm.png","key":"oleksm"}],"frontMatter":{"authors":["dshadravan","oleksm"],"date":"2024-09-10T00:00:00.000Z","slug":"open-connector","tags":["llm-open-connector"]},"unlisted":false,"prevItem":{"title":"About the Cookbook \ud83d\udcd6","permalink":"/einstein-platform/about"},"nextItem":{"title":"LLM Open Connector + Hugging Face","permalink":"/einstein-platform/huggingface"}},"content":"The LLM Open Connector is an [API specification](/docs/apis/llm-open-connector/), closely based on the OpenAI API, that lets you create API gateways and proxy servers that connect any language model to the Einstein AI Platform. Any service that implements the API specification can be connected to Einstein Studio using the Bring Your Own LLM (BYOLLM) feature.\\n\\nThe LLM Open Connector opens up a world of possibilities for bespoke AI applications for Salesforce customers and independent software vendors (ISVs). Check out our recipes to learn how to bring new models to Einstein Studio!"},{"id":"huggingface","metadata":{"permalink":"/einstein-platform/huggingface","source":"@site/cookbook/llm-open-connector-huggingface.mdx","title":"LLM Open Connector + Hugging Face","description":"Learn how to implement Salesforce\'s LLM Open Connector with the Hugging Face Serverless Inference API for LLM models that support Chat Completion. We also cover how to implement and deploy the connector as a Node.js app on Heroku.","date":"2024-09-12T00:00:00.000Z","tags":[{"inline":false,"label":"huggingface","permalink":"/einstein-platform/tags/huggingface"},{"inline":false,"label":"heroku","permalink":"/einstein-platform/tags/heroku"},{"inline":false,"label":"llm-open-connector","permalink":"/einstein-platform/tags/llm-open-connector"}],"readingTime":7.48,"hasTruncateMarker":true,"authors":[{"name":"Mohith","title":"Developer Advocate @ Salesforce","url":"https://github.com/msrivastav13","page":{"permalink":"/einstein-platform/authors/msrivastav-13"},"socials":{"github":"https://github.com/msrivastav13"},"imageURL":"https://github.com/msrivastav13.png","key":"msrivastav13"}],"frontMatter":{"slug":"huggingface","authors":["msrivastav13"],"tags":["huggingface","heroku","llm-open-connector"],"date":"2024-09-12T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Introducing the LLM Open Connector","permalink":"/einstein-platform/open-connector"},"nextItem":{"title":"LLM Open Connector + AWS","permalink":"/einstein-platform/aws"}},"content":"import ReactPlayer from \'react-player\'\\n\\n\\nLearn how to implement Salesforce\'s [LLM Open Connector](/docs/apis/llm-open-connector/) with the Hugging Face [Serverless Inference API](https://huggingface.co/docs/api-inference/index) for LLM models that support [Chat Completion](https://huggingface.co/docs/api-inference/en/tasks/chat-completion). We also cover how to implement and deploy the connector as a Node.js app on Heroku.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Prerequisites\\n\\nBefore you begin, make sure that your local environment meets these prerequisites:\\n\\n1. Node.js and npm are installed on your local machine.\\n2. You have a Heroku account. (Sign up at https://signup.heroku.com/.)\\n3. Heroku CLI is installed on your local machine (https://devcenter.heroku.com/articles/heroku-cli).\\n4. Git is installed on your local machine.\\n5. You have a Hugging Face account with an API key (https://huggingface.co/docs/hub/en/security-tokens).\\n\\n## Tutorial Video\\n\\nView a walkthrough on using any LLM with the Open Connector on the [Salesforce Developers](https://www.youtube.com/@SalesforceDevs) channel:\\n\\n<ReactPlayer playing={false} url=\'https://www.youtube.com/watch?v=CQdJxKZl0Y4\' />\\n\\n## Set Up Your Local Environment\\n\\n1. Clone the einstein-platform repository to your local environment.\\n   ```\\n    git clone https://github.com/salesforce/einstein-platform.git\\n    ```\\n\\n2. Create a directory for your project:\\n   ```\\n   mkdir llm-open-connector-hf\\n   cd llm-open-connector-hf\\n   ```\\n3. Locate the source files specific to Hugging Face connector. In the local `einstein-platform` project directory you cloned in step 1, navigate to the `documentation/cookbook-assets/llm-open-connector-huggingface` folder. Copy all the files in this folder.\\n\\n4.  Paste the `llm-open-connector-huggingface` files into the `llm-open-connector-hf` directory you created in step 2.\\n\\n5. Install the required dependencies.\\n   ```\\n   npm install\\n   ```\\n\\n## Project Structure\\n\\nThis section outlines the folder structure of the `llm-open-connector-hf` directory.\\n\\n   ```\\n    llm-open-connector-hf/\\n    \u251c\u2500\u2500 config/\\n    \u2502   \u2514\u2500\u2500 index.js\\n    \u251c\u2500\u2500 controllers/\\n    \u2502   \u2514\u2500\u2500 chatController.js\\n    \u251c\u2500\u2500 middleware/\\n    \u2502   \u2514\u2500\u2500 index.js\\n    \u251c\u2500\u2500 routes/\\n    \u2502   \u2514\u2500\u2500 chat.js\\n    \u251c\u2500\u2500 utils/\\n    \u2502   \u2514\u2500\u2500 logger.js\\n    \u251c\u2500\u2500 .gitignore\\n    \u251c\u2500\u2500 .prettierrc\\n    \u251c\u2500\u2500 index.js\\n    \u251c\u2500\u2500 package.json\\n    \u2514\u2500\u2500 Procfile\\n   ```\\n\\n\\n### Project Description\\n\\nThe following list provides a functional overview of each folder and file in the `llm-open-connector-hf` directory.\\n\\n- `config/`: Holds configuration settings.\\n  - `index.js`: Exports configuration options for different environments.\\n- `controllers/`: Manages the request handling logic for the chat API.\\n  - `chatController.js`: Calls the chat completion API.\\n- `middleware/`: Contains custom middleware functions.\\n  - `index.js`: Includes functions for API key validation and error handling.\\n- `routes/`: Defines API routes.\\n  - `chat.js`: Contains endpoints for chat completion functionality.\\n- `utils/`: Utility functions for support tasks.\\n  - `logger.js`: A custom logger with data sanitization for secure logging.\\n- `.gitignore`: Lists files and directories that are ignored by Git, such as `node_modules` and sensitive files.\\n- `.prettierrc`: The configuration file for Prettier, which enforces consistent code formatting across the project.\\n- `index.js`: The main application file that initializes the server and routes.\\n- `package.json`: Contains metadata for the project and manages dependencies, scripts, and configurations.\\n- `Procfile`: Specifies process types and commands for deploying the app on platforms like Heroku.\\n\\n\\n## Configure Your Local Environment\\n\\n1. Create a `.env` file in the root directory of the project and add these variables:\\n\\n   ```\\n   PORT=3000\\n   HUGGING_FACE_API_KEY=your_hugging_face_api_key\\n   ALLOWED_ORIGINS=http://localhost:3000,https://yourdomain.com\\n   ```\\n\\n   > Replace `your_hugging_face_api_key` with your own Hugging Face API key and adjust the `ALLOWED_ORIGINS` as needed. Change `ALLOWED_ORIGINS` to allowlist your Salesforce domain.\\n\\n2. Make sure your `.gitignore` file includes the `.env` file to avoid accidentally committing sensitive information.\\n\\n## Test Your Application Locally\\n\\n1. Run your Node.js application locally:\\n\\n   ```\\n   npm start\\n   ```\\n2. The server starts on the port specified in your `.env` file. The default port is 3000.\\n\\n3. Test the endpoints using a tool like cURL or Postman to ensure they\'re working correctly. To test the `chat/completions` endpoint with cURL, run the following command:\\n\\n    ```bash\\n    curl -X POST http://localhost:3000/chat/completions \\\\\\n    -H \'Content-Type: application/json\' \\\\\\n    -H \'api-key: <use your HuggingFace access token here>\' \\\\\\n    -d \'{\\n      \\"model\\": \\"mistralai/Mixtral-8x7B-Instruct-v0.1\\",\\n      \\"messages\\": [\\n        {\\n          \\"content\\": \\"What is the capital of Italy?\\",\\n          \\"role\\": \\"user\\"\\n        }\\n      ],\\n      \\"max_tokens\\": 100,\\n      \\"temperature\\": 0.7,\\n      \\"n\\": 2,\\n      \\"parameters\\": {\\n        \\"top_p\\": 0.9\\n      }\\n    }\'\\n    ```\\n\\n## Prepare for Heroku Deployment\\n\\n1. Initialize a Git repository in your project directory:\\n\\n   ```\\n   git init\\n   ```\\n\\n2. Add your files to the repository:\\n\\n   ```\\n   git add .\\n   ```\\n\\n3. Commit your changes:\\n   ```\\n   git commit -m \\"Initial commit\\"\\n   ```\\n\\n## Update Your Default Branch\\n\\nTo switch the default branch used to deploy apps from `master` to `main`, follow these steps:\\n\\n1. Create a branch locally:\\n\\n   ```bash\\n   git checkout -b main\\n   ```\\n\\n2. Delete the old default branch locally:\\n\\n   ```bash\\n   git branch -D master\\n   ```\\n\\n   Now, the local environment only knows about the `main` branch.\\n\\n3. Reset the git repository on the Heroku Platform:\\n\\n   - Use the `heroku-reset` command from the `heroku-repo` CLI plugin. This command does not impact the running application.\\n\\n   \\n   :::important\\n   Communicate this change with your team. If other developers are unaware of the reset and push to `master`, the reset will be overwritten.\\n   :::\\n\\n4. To switch the default branch in GitHub, refer to this article: [Setting the Default Branch](https://docs.github.com/en/github/administering-a-repository/setting-the-default-branch).\\n\\n## Deploy to Heroku\\n\\n1. Login to the Heroku CLI:\\n\\n   ```\\n   heroku login\\n   ```\\n\\n2. Create a Heroku app:\\n\\n   ```\\n   heroku create your-app-name\\n   ```\\n\\n   Replace `your-app-name` with a unique name for your application.\\n\\n3. Set the API_KEY config var on Heroku:\\n\\n   ```\\n   heroku config:set HUGGING_FACE_API_KEY=your_api_key_here -a your-app-name\\n   ```\\n\\n   Replace `your_api_key_here` with your own API key.\\n\\n4. Set the ALLOWED_ORIGINS config var on Heroku:\\n\\n    ```\\n    heroku config:set ALLOWED_ORIGINS=your_salesforce_domain_here -a your-app-name\\n    ```\\n\\n    Replace `your_salesforce_domain_here` with your own Salesforce domain.\\n\\n5. Deploy your app to Heroku:\\n\\n   ```\\n   git push heroku main\\n   ```\\n\\n## Test Your Deployed Application\\n\\nYou can test your deployed application using the deployed API endpoint. Use a tool like cURL or Postman to test the endpoints of your Node.js app:\\n\\n   - Chat Completions: `POST https://your-app-name.herokuapp.com/chat/completions`\\n\\nTo test the `chat/completions` endpoint with cURL, run this command:\\n\\n```bash\\ncurl -X POST https://still-beach-80840-58cbb07ae5f4.herokuapp.com/chat/completions \\\\\\n-H \'Content-Type: application/json\' \\\\\\n-H \'api-key: <use your HuggingFace access token here>\' \\\\\\n-d \'{\\n  \\"model\\": \\"mistralai/Mixtral-8x7B-Instruct-v0.1\\",\\n  \\"messages\\": [\\n    {\\n      \\"content\\": \\"What is the capital of Italy?\\",\\n      \\"role\\": \\"user\\"\\n    }\\n  ],\\n  \\"max_tokens\\": 100,\\n  \\"temperature\\": 0.7,\\n  \\"n\\": 2,\\n  \\"parameters\\": {\\n    \\"top_p\\": 0.9\\n  }\\n}\'\\n```\\n## Bring Your Connected Endpoint to Salesforce Model Builder\\n\\nFollow the instructions in [this developer blog](https://developer.salesforce.com/blogs/2024/10/build-generative-ai-solutions-with-llm-open-connector) to use your model in Model Builder. When you activate your model, you can use it within [Prompt Builder](https://developer.salesforce.com/docs/einstein/genai/guide/get-started-prompt-builder.html), the [Models API](https://developer.salesforce.com/docs/einstein/genai/guide/models-api.html), and for building actions using prompt templates in Agent Builder. All these methods provide built-in security offered by the [Einstein Trust Layer](https://help.salesforce.com/s/articleView?id=sf.generative_ai_trust_layer.htm&type=5).\\n\\n## Current Features\\nThe following list outlines the features included in this recipe:\\n- Integration with the [Hugging Face](https://huggingface.co/docs/api-inference/index) Serverless Inference API for [models](https://huggingface.co/docs/api-inference/en/tasks/chat-completion) that support Chat Completion\\n- Express server with advanced security configurations\\n- CORS configuration with customizable allowed origins\\n- Rate limiting to prevent abuse\\n- API key validation for protected routes\\n- Comprehensive error handling and sanitized logging\\n- Helmet.js integration for enhanced security headers\\n- Chat completion controller with input validation and response reshaping\\n- Optimized message processing:\\n  - Concatenates multiple system messages into a single message as required by some LLMs\\n  - Preserves the order of user and assistant messages\\n\\n## API Endpoints\\n\\n- POST `/chat/completions`: Send a chat message and receive an AI-generated response.\\n  - The endpoint optimizes message processing by concatenating system messages.\\n  - The following example provides two system messages:\\n    ```json\\n    {\\n      \\"messages\\": [\\n        {\\"role\\": \\"system\\", \\"content\\": \\"You are a helpful assistant.\\"},\\n        {\\"role\\": \\"system\\", \\"content\\": \\"Always be polite.\\"},\\n        {\\"role\\": \\"user\\", \\"content\\": \\"Hello!\\"},\\n        {\\"role\\": \\"assistant\\", \\"content\\": \\"Hi there!\\"},\\n        {\\"role\\": \\"user\\", \\"content\\": \\"How are you?\\"}\\n      ],\\n      \\"model\\": \\"gpt-3.5-turbo\\",\\n      \\"max_tokens\\": 150\\n    }\\n    ```\\n  - The API processes the system messages into:\\n    ```json\\n    {\\n      \\"messages\\": [\\n        {\\"role\\": \\"system\\", \\"content\\": \\"You are a helpful assistant.\\\\nAlways be polite.\\"},\\n        {\\"role\\": \\"user\\", \\"content\\": \\"Hello!\\"},\\n        {\\"role\\": \\"assistant\\", \\"content\\": \\"Hi there!\\"},\\n        {\\"role\\": \\"user\\", \\"content\\": \\"How are you?\\"}\\n      ],\\n      \\"model\\": \\"gpt-3.5-turbo\\",\\n      \\"max_tokens\\": 150\\n    }\\n    ```\\n\\n## Current Security Measures\\nThe recipe\'s security measures include a Helmet.js configuration, a CORS configuration, rate limiting, API key validation, and sanitized logging.\\n\\nThe following list briefly highlights the current security measures: \\n- A Helmet.js configuration with strict security settings:\\n  - Content Security Policy (CSP) with restrictive directives\\n  - Cross-Origin Embedder Policy\\n  - Cross-Origin Opener Policy\\n  - Cross-Origin Resource Policy\\n  - DNS Prefetch Control\\n  - Expect-CT header\\n  - Frameguard to prevent clickjacking\\n  - HTTP Strict Transport Security (HSTS)\\n  - IE No Open\\n  - X-Content-Type-Options nosniff\\n  - Origin-Agent-Cluster header\\n  - Permitted Cross-Domain Policies\\n  - Referrer-Policy\\n  - X-XSS-Protection\\n- A CORS configuration restricts allowed origins.\\n- Rate limits: 100 requests per 15 minutes per IP.\\n- API key validation provides protected routes.\\n- Sanitized logging prevents accidental exposure of sensitive data.\\n\\n## Important Considerations\\n\\n1. This cookbook uses the `mistralai/Mixtral-8x7B-Instruct-v0.1` model in the example cURL requests. You can replace it with any supported model. For a list of supported model IDs, see [Hugging Face Chat Completion](https://huggingface.co/docs/api-inference/en/tasks/chat-completion).\\n2. Monitor your usage to manage costs, especially if you expect high traffic.\\n3. Error handling and input validation must be improved for production use.\\n4. Consider implementing further security measures for production use.\\n\\n## Conclusion\\nThis cookbook demonstrates how to set up an LLM Open Connector using Hugging Face Serverless API for Chat Completion endpoints for various models. For production purposes, remember to use Hugging Face\'s dedicated [Interference Endpoints](https://ui.endpoints.huggingface.co/) feature. Always optimize your implementation based on your specific requirements and expected usage patterns."},{"id":"aws","metadata":{"permalink":"/einstein-platform/aws","source":"@site/cookbook/llm-open-connector-aws.md","title":"LLM Open Connector + AWS","description":"Learn how to implement Salesforce\'s LLM Open Connector using Amazon Web Services (AWS). We use Amazon Bedrock to host a Llama model from Meta, a Lambda for serverless compute, and an API Gateway to expose our API.","date":"2024-09-13T00:00:00.000Z","tags":[{"inline":false,"label":"aws","permalink":"/einstein-platform/tags/aws"},{"inline":false,"label":"bedrock","permalink":"/einstein-platform/tags/bedrock"},{"inline":false,"label":"llm-open-connector","permalink":"/einstein-platform/tags/llm-open-connector"}],"readingTime":4.24,"hasTruncateMarker":true,"authors":[{"name":"Richard","title":"Technical Writer @ Salesforce","url":"https://github.com/rsexton404","page":{"permalink":"/einstein-platform/authors/rsexton"},"socials":{"github":"https://github.com/rsexton404"},"imageURL":"https://github.com/rsexton404.png","key":"rsexton"}],"frontMatter":{"slug":"aws","authors":["rsexton"],"tags":["aws","bedrock","llm-open-connector"],"date":"2024-09-13T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"LLM Open Connector + Hugging Face","permalink":"/einstein-platform/huggingface"},"nextItem":{"title":"LLM Open Connector + Groq","permalink":"/einstein-platform/groq"}},"content":"Learn how to implement Salesforce\'s [LLM Open Connector](/docs/apis/llm-open-connector/) using Amazon Web Services (AWS). We use Amazon Bedrock to host a Llama model from Meta, a Lambda for serverless compute, and an API Gateway to expose our API.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Prerequisites\\n\\n1. AWS Account with appropriate permissions\\n2. AWS CLI. For installation instructions, see [Install or update to the latest version of the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) and [Set up the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-quickstart.html).\\n3. Python 3.9 or later\\n4. AWS CDK installed (`npm install -g aws-cdk`)\\n5. Access to AWS Bedrock (you may need to request access if it\'s not enabled in your account)\\n\\n## Set Up AWS Bedrock and Model Access\\n\\nBefore deploying and using the LLM Open Connector, you need to set up access to AWS Bedrock and enable the specific model you want to use. Follow these steps:\\n\\n**Enable AWS Bedrock:**\\n\\n- Go to the [AWS Bedrock console](https://console.aws.amazon.com/bedrock).\\n- If you see a \\"Get started\\" button, click it to enable Bedrock for your account.\\n- If Bedrock is already enabled, you\'ll see the Bedrock dashboard.\\n\\n**Request access to models:**\\n\\n- In the Bedrock console, go to \\"Model access\\" in the left navigation pane.\\n- Find the model you want to use (for example, Llama 3.1 70B Instruct).\\n- Click the menu (three dots) and select \\"Request model access\\".\\n- Verify that the model is selected and click **Next**.\\n- Read and accept the terms and conditions, then click **Submit**.\\n- Wait for your request to be approved. This is usually instant for most models.\\n\\n## Installation\\n\\n1. Create a new local directory for your project and navigate to it:\\n\\n   ```\\n   mkdir aws-bedrock-llama-connector\\n   cd aws-bedrock-llama-connector\\n   ```\\n\\n2. Download from the einstein-platform repository:\\n\\n   - [lib/aws_bedrock_llama_connector_stack.py](https://github.com/salesforce/einstein-platform/tree/main/documentation/cookbook-assets/llm-open-connector-aws/lib/aws_bedrock_llama_connector_stack.py)\\n   - [lambda/handler.py](https://github.com/salesforce/einstein-platform/tree/main/documentation/cookbook-assets/llm-open-connector-aws/lambda/handler.py)\\n   - [app.py](https://github.com/salesforce/einstein-platform/tree/main/documentation/cookbook-assets/llm-open-connector-aws/app.py)\\n   - [cdk.json](https://github.com/salesforce/einstein-platform/tree/main/documentation/cookbook-assets/llm-open-connector-aws/cdk.json)\\n   - [index.html](https://github.com/salesforce/einstein-platform/tree/main/documentation/cookbook-assets/llm-open-connector-aws/index.html)\\n   - [requirements.txt](https://github.com/salesforce/einstein-platform/tree/main/documentation/cookbook-assets/llm-open-connector-aws/requirements.txt)\\n\\n3. Copy the downloaded files into your project directory.\\n\\n4. Create a virtual environment and install Python dependencies:\\n\\n   ```\\n   python -m venv venv\\n   source venv/bin/activate  # On Windows, use `venv\\\\Scripts\\\\activate`\\n   pip install -r requirements.txt\\n   ```\\n\\n## Project Structure\\n\\nYour project structure should now look like this:\\n\\n```\\naws-bedrock-llama-connector/\\n\u251c\u2500\u2500 lib/\\n\u2502   \u2514\u2500\u2500 aws_bedrock_llama_connector_stack.py\\n\u251c\u2500\u2500 lambda/\\n\u2502   \u2514\u2500\u2500 handler.py\\n\u251c\u2500\u2500 app.py\\n\u251c\u2500\u2500 cdk.json\\n\u251c\u2500\u2500 index.html\\n\u2514\u2500\u2500 requirements.txt\\n```\\n\\n## Deployment\\n\\n1. Configure AWS CLI with your credentials:\\n\\n   ```\\n   aws configure\\n   ```\\n\\n2. Bootstrap your AWS environment (if you haven\'t already):\\n\\n   ```\\n   cdk bootstrap\\n   ```\\n\\n3. Deploy the stack:\\n\\n   ```\\n   cdk deploy\\n   ```\\n\\n4. After deployment, CDK will output the API Gateway URL. You can use this URL to make requests to your Llama model.\\n\\n## Usage\\n\\nTo use the deployed API, send a POST request to the `/chat/completions` endpoint with the following JSON structure:\\n\\n```json\\n{\\n  \\"messages\\": [\\n    { \\"role\\": \\"system\\", \\"content\\": \\"You are a helpful assistant.\\" },\\n    { \\"role\\": \\"user\\", \\"content\\": \\"Tell me a joke about programming.\\" }\\n  ],\\n  \\"temperature\\": 0.7\\n}\\n```\\n\\nYou can use tools like cUrl, Postman, or any programming language to make HTTP requests to your API.\\n\\n### Example using cUrl\\n\\nHere\'s an example cUrl command to test your API:\\n\\n```bash\\ncurl -X POST https://your-api-id.execute-api.your-region.amazonaws.com/prod/chat/completions \\\\\\n-H \\"Content-Type: application/json\\" \\\\\\n-d \'{\\n  \\"messages\\": [\\n    {\\"role\\": \\"system\\", \\"content\\": \\"You are a helpful assistant.\\"},\\n    {\\"role\\": \\"user\\", \\"content\\": \\"Tell me a joke about programming.\\"}\\n  ],\\n  \\"temperature\\": 0.7\\n}\'\\n```\\n\\nReplace `https://your-api-id.execute-api.your-region.amazonaws.com/prod` with the actual URL of your deployed API Gateway, which you can find in the AWS Console or in the output of the `cdk deploy` command.\\n\\nThis cUrl command sends a POST request to your API with the specified JSON payload. The API should respond with a JSON object containing the generated text from the Llama model.\\n\\n**Note**: If you\'re using Windows Command Prompt, you may need to escape the double quotes in the JSON payload differently. For PowerShell or bash on Windows (e.g., Git Bash), the command should work as written above.\\n\\n## Testing with a Web UI\\n\\nUse the included [index.html](https://github.com/salesforce/einstein-platform/tree/main/documentation/cookbook-assets/llm-open-connector-aws/index.html) file to test your API gateway with a web UI.\\n\\nBefore using the web UI, edit the file and replace `https://your-api-id.execute-api.your-region.amazonaws.com/prod` with the actual URL of your deployed API Gateway, which you can find in the AWS Console or in the output of the `cdk deploy` command.\\n\\n## Notes\\n\\n1. This cookbook uses the `meta.llama3-1-70b-instruct-v1:0` model as a placeholder. You can replace it with any supported model that you have been granted access to in Amazon Bedrock. For a list of supported model IDs, see [Amazon Bedrock model IDs](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html).\\n2. The token count in the response is set to -1 as AWS Bedrock doesn\'t provide token usage information. You may need to implement your own token counting logic if required.\\n3. Error handling and input validation should be improved for production use.\\n4. Consider disabling CORS.\\n5. Consider implementing authentication and rate limiting for your API Gateway.\\n6. Monitor your AWS usage to manage costs, especially if you expect high traffic.\\n\\n## Conclusion\\n\\nThis cookbook demonstrates how to set up an LLM Open Connector using AWS Bedrock, Lambda, and API Gateway. It provides a serverless, scalable solution for integrating Llama models into your applications. Remember to optimize your implementation based on your specific requirements and expected usage patterns."},{"id":"groq","metadata":{"permalink":"/einstein-platform/groq","source":"@site/cookbook/llm-open-connector-groq.md","title":"LLM Open Connector + Groq","description":"Learn how to implement Salesforce\'s LLM Open Connector with the Groq platform for fast AI inference. We also cover how to deploy the connector as a Flask app on Heroku with a simple web UI for testing.","date":"2024-09-14T00:00:00.000Z","tags":[{"inline":false,"label":"groq","permalink":"/einstein-platform/tags/groq"},{"inline":false,"label":"heroku","permalink":"/einstein-platform/tags/heroku"},{"inline":false,"label":"llm-open-connector","permalink":"/einstein-platform/tags/llm-open-connector"}],"readingTime":3.765,"hasTruncateMarker":true,"authors":[{"name":"Richard","title":"Technical Writer @ Salesforce","url":"https://github.com/rsexton404","page":{"permalink":"/einstein-platform/authors/rsexton"},"socials":{"github":"https://github.com/rsexton404"},"imageURL":"https://github.com/rsexton404.png","key":"rsexton"}],"frontMatter":{"slug":"groq","authors":["rsexton"],"tags":["groq","heroku","llm-open-connector"],"date":"2024-09-14T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"LLM Open Connector + AWS","permalink":"/einstein-platform/aws"},"nextItem":{"title":"LLM Open Connector + SambaNova","permalink":"/einstein-platform/sambanova"}},"content":"Learn how to implement Salesforce\'s [LLM Open Connector](/docs/apis/llm-open-connector/) with the [Groq](https://groq.com/) platform for fast AI inference. We also cover how to deploy the connector as a Flask app on Heroku with a simple web UI for testing.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Prerequisites\\n\\nBefore you begin, make sure that your local environment meets these prerequisites.\\n\\n1. Python 3.9 or later installed on your local machine\\n2. A Heroku account (sign up at https://signup.heroku.com/)\\n3. Heroku CLI installed (https://devcenter.heroku.com/articles/heroku-cli)\\n4. Git installed on your local machine\\n5. A Groq API key (sign up at https://console.groq.com/)\\n\\n## Set Up Your Local Environment\\n\\n1. Create a new directory for your project:\\n\\n   ```\\n   mkdir llm-open-connector\\n   cd llm-open-connector\\n   ```\\n\\n2. Create a virtual environment and activate it:\\n\\n   ```\\n   python -m venv venv\\n   source venv/bin/activate  # On Windows, use `venv\\\\Scripts\\\\activate`\\n   ```\\n\\n3. Download these files from the einstein-platform repository:\\n\\n   - [.gitignore](https://github.com/salesforce/einstein-platform/tree/main/documentation/cookbook-assets/llm-open-connector-groq/.gitignore)\\n   - [app.py](https://github.com/salesforce/einstein-platform/tree/main/documentation/cookbook-assets/llm-open-connector-groq/app.py)\\n   - [index.html](https://github.com/salesforce/einstein-platform/tree/main/documentation/cookbook-assets/llm-open-connector-groq/index.html)\\n   - [Procfile](https://github.com/salesforce/einstein-platform/tree/main/documentation/cookbook-assets/llm-open-connector-groq/Procfile)\\n   - [requirements.txt](https://github.com/salesforce/einstein-platform/tree/main/documentation/cookbook-assets/llm-open-connector-groq/requirements.txt)\\n   - [runtime.txt](https://github.com/salesforce/einstein-platform/tree/main/documentation/cookbook-assets/llm-open-connector-groq/runtime.txt)\\n\\n4. Copy the downloaded files into your project directory.\\n\\n5. Install the required packages:\\n   ```\\n   pip install -r requirements.txt\\n   ```\\n\\n## Configure Your Local Environment\\n\\n1. For local testing, create a `.env` file in your project directory and add your Groq API key:\\n\\n   ```\\n   GROQ_API_KEY=your_groq_api_key_here\\n   ```\\n\\n   Replace `your_groq_api_key_here` with your actual Groq API key.\\n\\n2. Make sure your `.gitignore` file includes the `.env` file to avoid accidentally committing sensitive information.\\n\\n## Test Your Application Locally\\n\\n1. Run your Flask application:\\n\\n   ```\\n   python app.py\\n   ```\\n\\n2. Your app should now be running on `http://127.0.0.1:5000/`.\\n\\n3. Test the endpoints using a tool like cURL or Postman to ensure they\'re working correctly.\\n\\nTo test the `chat/completions` endpoint, run this cURL command:\\n\\n```bash\\ncurl -X POST http://127.0.0.1:5000/chat/completions \\\\\\n-H \\"Content-Type: application/json\\" \\\\\\n-d \'{\\n  \\"model\\": \\"llama3-8b-8192\\",\\n  \\"messages\\": [\\n    {\\"role\\": \\"system\\", \\"content\\": \\"You are a helpful assistant.\\"},\\n    {\\"role\\": \\"user\\", \\"content\\": \\"What is the capital of Canada?\\"}\\n  ],\\n  \\"max_tokens\\": 50,\\n  \\"temperature\\": 0.7,\\n  \\"n\\": 1\\n}\'\\n```\\n\\n## Prepare for Heroku Deployment\\n\\n1. Initialize a Git repository in your project directory:\\n\\n   ```\\n   git init\\n   ```\\n\\n2. Add your files to the repository:\\n\\n   ```\\n   git add .\\n   ```\\n\\n3. Commit your changes:\\n   ```\\n   git commit -m \\"Initial commit\\"\\n   ```\\n\\n## Update Your Default Branch\\n\\nTo switch the default branch used to deploy apps from `master` to `main`, follow these steps:\\n\\n1. Create a new branch locally:\\n\\n   ```bash\\n   git checkout -b main\\n   ```\\n\\n2. Delete the old default branch locally:\\n\\n   ```bash\\n   git branch -D master\\n   ```\\n\\n   Now, the local environment only knows about the `main` branch.\\n\\n3. Reset the git repository on the Heroku Platform:\\n\\n   - Use the `heroku-reset` command from the `heroku-repo` CLI plugin.\\n   - This will not impact the running application.\\n\\n   **Note:** Communicate this change with your team. If other developers are unaware of the reset, they might push to `master`, overwriting the reset.\\n\\n4. To switch the default branch in GitHub, refer to this article: [Setting the Default Branch](https://docs.github.com/en/github/administering-a-repository/setting-the-default-branch).\\n\\n## Deploy to Heroku\\n\\n1. Make sure you\'re logged in to the Heroku CLI:\\n\\n   ```\\n   heroku login\\n   ```\\n\\n2. Create a new Heroku app:\\n\\n   ```\\n   heroku create your-app-name\\n   ```\\n\\n   Replace `your-app-name` with a unique name for your application.\\n\\n3. Set the GROQ_API_KEY config var on Heroku:\\n\\n   ```\\n   heroku config:set GROQ_API_KEY=your_groq_api_key_here -a your-app-name\\n   ```\\n\\n   Replace `your_groq_api_key_here` with your actual Groq API key.\\n\\n4. Deploy your app to Heroku:\\n\\n   ```\\n   git push heroku main\\n   ```\\n\\n5. Open your deployed app:\\n   ```\\n   heroku open\\n   ```\\n\\nYour LLM Open Connector should now be deployed and accessible via the Heroku URL.\\n\\n## Test Your Deployed Application\\n\\nYou can test your deployed application in two ways:\\n\\n1. Using the example UI:\\n\\n   - Open your browser and navigate to `https://your-app-name.herokuapp.com`\\n   - You\'ll see a simple interface where you can input prompts and get responses from the LLM\\n   - Try different prompts and get super fast responses!\\n\\n2. Using API endpoints:\\n   Use a tool like cURL or Postman to test the endpoints of your Flask app:\\n   - Chat Completions: `POST https://your-app-name.herokuapp.com/chat/completions`\\n\\n## Conclusion\\n\\nYou have successfully created and deployed an LLM Open Connector using the Groq API and deployed it to Heroku! This connector adheres to the Salesforce LLM Open Connector API specification, allowing for seamless integration with the Einstein AI Platform using the BYOLLM feature.\\n\\nWith this connector, you can bring new foundation models like Llama 3 into Einstein Studio that take advantage of Groq\'s fast inference platform.\\n\\nRemember to monitor your usage and costs associated with the Groq API, and consider implementing additional security measures, such as rate limiting, CORS restrictions, and user authentication, before using this connector in a production environment."},{"id":"sambanova","metadata":{"permalink":"/einstein-platform/sambanova","source":"@site/cookbook/llm-open-connector-sambanova.md","title":"LLM Open Connector + SambaNova","description":"Learn how to implement Salesforce\'s LLM Open Connector with the SambaNova platform for fast AI inference. We also cover how to deploy the connector as a Flask app on Heroku with a simple web UI for testing.","date":"2024-09-15T00:00:00.000Z","tags":[{"inline":false,"label":"heroku","permalink":"/einstein-platform/tags/heroku"},{"inline":false,"label":"llm-open-connector","permalink":"/einstein-platform/tags/llm-open-connector"},{"inline":false,"label":"sambanova","permalink":"/einstein-platform/tags/sambanova"}],"readingTime":3.73,"hasTruncateMarker":true,"authors":[{"name":"Richard","title":"Technical Writer @ Salesforce","url":"https://github.com/rsexton404","page":{"permalink":"/einstein-platform/authors/rsexton"},"socials":{"github":"https://github.com/rsexton404"},"imageURL":"https://github.com/rsexton404.png","key":"rsexton"}],"frontMatter":{"slug":"sambanova","authors":["rsexton"],"tags":["heroku","llm-open-connector","sambanova"],"date":"2024-09-15T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"LLM Open Connector + Groq","permalink":"/einstein-platform/groq"},"nextItem":{"title":"LLM Open Connector + watsonx","permalink":"/einstein-platform/ibm"}},"content":"Learn how to implement Salesforce\'s [LLM Open Connector](/docs/apis/llm-open-connector/) with the [SambaNova](https://sambanova.ai/) platform for fast AI inference. We also cover how to deploy the connector as a Flask app on Heroku with a simple web UI for testing.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Prerequisites\\n\\nBefore you begin, make sure that your local environment meets these prerequisites.\\n\\n1. Python 3.9 or later installed on your local machine\\n2. A Heroku account (sign up at https://signup.heroku.com/)\\n3. Heroku CLI installed (https://devcenter.heroku.com/articles/heroku-cli)\\n4. Git installed on your local machine\\n5. A SambaNova API key (sign up at https://sambanova.ai/)\\n\\n## Set Up Your Local Environment\\n\\n1. Create a new directory for your project:\\n\\n   ```\\n   mkdir llm-open-connector\\n   cd llm-open-connector\\n   ```\\n\\n2. Create a virtual environment and activate it:\\n\\n   ```\\n   python -m venv venv\\n   source venv/bin/activate  # On Windows, use `venv\\\\Scripts\\\\activate`\\n   ```\\n\\n3. Download these files from the einstein-platform repository:\\n\\n   - [.gitignore](https://github.com/salesforce/einstein-platform/tree/main/documentation/cookbook-assets/llm-open-connector-sambanova/.gitignore)\\n   - [app.py](https://github.com/salesforce/einstein-platform/tree/main/documentation/cookbook-assets/llm-open-connector-sambanova/app.py)\\n   - [index.html](https://github.com/salesforce/einstein-platform/tree/main/documentation/cookbook-assets/llm-open-connector-sambanova/index.html)\\n   - [Procfile](https://github.com/salesforce/einstein-platform/tree/main/documentation/cookbook-assets/llm-open-connector-sambanova/Procfile)\\n   - [requirements.txt](https://github.com/salesforce/einstein-platform/tree/main/documentation/cookbook-assets/llm-open-connector-sambanova/requirements.txt)\\n   - [runtime.txt](https://github.com/salesforce/einstein-platform/tree/main/documentation/cookbook-assets/llm-open-connector-sambanova/runtime.txt)\\n\\n4. Copy the downloaded files into your project directory.\\n\\n5. Install the required packages:\\n   ```\\n   pip install -r requirements.txt\\n   ```\\n\\n## Configure Your Local Environment\\n\\n1. For local testing, create a `.env` file in your project directory and add your API key:\\n\\n   ```\\n   API_KEY=your_api_key_here\\n   ```\\n\\n   Replace `your_api_key_here` with your actual API key.\\n\\n2. Make sure your `.gitignore` file includes the `.env` file to avoid accidentally committing sensitive information.\\n\\n## Test Your Application Locally\\n\\n1. Run your Flask application:\\n\\n   ```\\n   python app.py\\n   ```\\n\\n2. Your app should now be running on `http://127.0.0.1:5000/`.\\n\\n3. Test the endpoints using a tool like cURL or Postman to ensure they\'re working correctly.\\n\\nTo test the `chat/completions` endpoint, run this cURL command:\\n\\n```bash\\ncurl -X POST http://127.0.0.1:5000/chat/completions \\\\\\n-H \\"Content-Type: application/json\\" \\\\\\n-d \'{\\n  \\"model\\": \\"Meta-Llama-3.1-8B-Instruct\\",\\n  \\"messages\\": [\\n    {\\"role\\": \\"system\\", \\"content\\": \\"You are a helpful assistant.\\"},\\n    {\\"role\\": \\"user\\", \\"content\\": \\"What is the capital of Canada?\\"}\\n  ],\\n  \\"temperature\\": 0.7,\\n}\'\\n```\\n\\n## Prepare for Heroku Deployment\\n\\n1. Initialize a Git repository in your project directory:\\n\\n   ```\\n   git init\\n   ```\\n\\n2. Add your files to the repository:\\n\\n   ```\\n   git add .\\n   ```\\n\\n3. Commit your changes:\\n   ```\\n   git commit -m \\"Initial commit\\"\\n   ```\\n\\n## Update Your Default Branch\\n\\nTo switch the default branch used to deploy apps from `master` to `main`, follow these steps:\\n\\n1. Create a new branch locally:\\n\\n   ```bash\\n   git checkout -b main\\n   ```\\n\\n2. Delete the old default branch locally:\\n\\n   ```bash\\n   git branch -D master\\n   ```\\n\\n   Now, the local environment only knows about the `main` branch.\\n\\n3. Reset the git repository on the Heroku Platform:\\n\\n   - Use the `heroku-reset` command from the `heroku-repo` CLI plugin.\\n   - This will not impact the running application.\\n\\n   **Note:** Communicate this change with your team. If other developers are unaware of the reset, they might push to `master`, overwriting the reset.\\n\\n4. To switch the default branch in GitHub, refer to this article: [Setting the Default Branch](https://docs.github.com/en/github/administering-a-repository/setting-the-default-branch).\\n\\n## Deploy to Heroku\\n\\n1. Make sure you\'re logged in to the Heroku CLI:\\n\\n   ```\\n   heroku login\\n   ```\\n\\n2. Create a new Heroku app:\\n\\n   ```\\n   heroku create your-app-name\\n   ```\\n\\n   Replace `your-app-name` with a unique name for your application.\\n\\n3. Set the API_KEY config var on Heroku:\\n\\n   ```\\n   heroku config:set API_KEY=your_api_key_here -a your-app-name\\n   ```\\n\\n   Replace `your_api_key_here` with your actual API key.\\n\\n4. Deploy your app to Heroku:\\n\\n   ```\\n   git push heroku main\\n   ```\\n\\n5. Open your deployed app:\\n   ```\\n   heroku open\\n   ```\\n\\nYour LLM Open Connector should now be deployed and accessible via the Heroku URL.\\n\\n## Test Your Deployed Application\\n\\nYou can test your deployed application in two ways:\\n\\n1. Using the example UI:\\n\\n   - Open your browser and navigate to `https://your-app-name.herokuapp.com`\\n   - You\'ll see a simple interface where you can input prompts and get responses from the LLM\\n   - Try different prompts and get super fast responses!\\n\\n2. Using API endpoints:\\n   Use a tool like cURL or Postman to test the endpoints of your Flask app:\\n\\n   - Chat Completions: `POST https://your-app-name.herokuapp.com/chat/completions`\\n\\n## Conclusion\\n\\nYou have successfully created and deployed an LLM Open Connector using the SambaNova API and deployed it to Heroku! This connector adheres to the Salesforce LLM Open Connector API specification, allowing for seamless integration with the Einstein AI Platform using the BYOLLM feature.\\n\\nWith this connector, you can bring new foundation models like Llama 3 into Einstein Studio that take advantage of SambaNova\'s fast inference platform.\\n\\nRemember to monitor your usage and costs associated with the SambaNova API, and consider implementing additional security measures, such as rate limiting, CORS restrictions, and user authentication, before using this connector in a production environment."},{"id":"ibm","metadata":{"permalink":"/einstein-platform/ibm","source":"@site/cookbook/llm-open-connector-ibm.md","title":"LLM Open Connector + watsonx","description":"Learn how to implement Salesforce\'s LLM Open Connector with the IBM watsonx platform.","date":"2024-10-18T00:00:00.000Z","tags":[{"inline":false,"label":"ibm","permalink":"/einstein-platform/tags/ibm"},{"inline":false,"label":"llm-open-connector","permalink":"/einstein-platform/tags/llm-open-connector"}],"readingTime":3.395,"hasTruncateMarker":true,"authors":[{"name":"Tate","title":"Technical Writer @ Salesforce","url":"https://github.com/tatedorman","page":{"permalink":"/einstein-platform/authors/tatedorman"},"socials":{"github":"https://github.com/tatedorman"},"imageURL":"https://github.com/tatedorman.png","key":"tatedorman"}],"frontMatter":{"slug":"ibm","authors":["tatedorman"],"tags":["ibm","llm-open-connector"],"date":"2024-10-18T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"LLM Open Connector + SambaNova","permalink":"/einstein-platform/sambanova"},"nextItem":{"title":"LLM Open Connector + MuleSoft + Cerebras","permalink":"/einstein-platform/mulesoft"}},"content":"Learn how to implement Salesforce\'s [LLM Open Connector](/docs/apis/llm-open-connector/) with the IBM watsonx platform.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Prerequisites\\n\\n- A Salesforce org with Einstein Generative AI and Data Cloud enabled.\\n- A Salesforce Einstein Studio Environment.\\n\\n## Step 1\\\\. Create Your watsonx Instance\\n\\n**Create an account.**\\n\\n1. If you don\u2019t already have one, [create a watsonx trial account](https://dataplatform.cloud.ibm.com/registration/stepone?context=wx). \\n\\n   If you have an existing, non-trial watsonx account, you need to follow these additional steps: \\n\\n    1. In IBM Cloud, [set up IBM Cloud Object Storage for use with IBM watsonx](https://dataplatform.cloud.ibm.com/docs/content/wsj/console/wdp_admin_cos.html?context=wx&audience=wdp).   \\n    2. [Set up the Watson Studio and Watson Machine Learning](https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/set-up-ws.html?context=wx&audience=wdp) services.   \\n    3. Create a new project from the [IBM watsonx console](https://dataplatform.cloud.ibm.com/projects/?context=wx).\\n\\n**Find your Project ID value.**\\n\\n1. In your sandbox, click the **Manage** tab and copy your Project ID value.   \\n2. Store the project-id value. You\u2019ll need this value along with an API key and a region-id in step three.\\n\\n**Create an IBM Cloud API Key.**\\n\\n1. In the IBM Cloud console, go to **Manage** \\\\> **Access (IAM)**.  \\n2. In the sidebar, select **API Keys**.  \\n3. Click **Create**.  \\n4. Enter a name and description for your API key.  \\n5. Then, click **Create**.  \\n6. Store your API key in a safe location. \\n\\n**Get the region value for the watsonx instance.**\\n\\n1. From the watsonx home page, check the region in the header bar. \\n\\n![IBM region id value](/img/ibm-region-id.png)\\n\\n2. Store the region-id value.\\n\\n## Step 2\\\\. Verify Your Open Connector Implementation (Optional)\\n\\nIBM automatically provides a working implementation of the Open Connector for watsonx that you can use to test this workflow. If you want to create your own implementation for production use cases, follow these steps. Otherwise, skip to step three.\\n\\n1. Create your own connector implementation for watsonx. For directions and code, see [IBM\'s watsonx connector repo](https://ibm.biz/eebl-salesforce-watsonx-apic-connector-code).\\n\\n## Step 3\\\\. Create a BYOLLM Connection to the watsonx Model in Einstein Studio\\n\\nBefore you connect your Open Connector implementation to Einstein Studio, you need three pieces of information from Step 1: project-id, API key, and region-id.\\n\\n1. Log in to your Salesforce org as an admin user and open Einstein Studio.  \\n2. Click **Model Library**.  \\n3. Click **Add Foundation Model**. \\n\\n![Add a Foundation Model](/img/ibm-model-library.png)\\n\\n4. In Model Builder, click **Connect to your LLM**.\\n\\n![Connect to your LLM](/img/ibm-model-builder.png)\\n\\n5. Click **Next**.  \\n6. Enter the details of your watsonx instance.   \\n   * Name: IBM Granite (or your own preferred name)  \\n   * URL:  if you are using the connector hosted by IBM, get the URL from [IBM\'s watsonx connector repo](https://ibm.biz/eebl-salesforce-watsonx-apic-connector-code). Otherwise, use the URL from the connector instance that you have created from the code and documentation in [the watsonx repo](https://ibm.biz/eebl-salesforce-watsonx-apic-connector-code). When filling out the URL value, be sure to use the project-id and region-id that you copied from step 1\\\\.  \\n   * Authentication: Key Based  \\n   * Auth Header: X-IBM-API-KEY  \\n   * Key: \\\\[your IBM API key\\\\]  \\n   * Model Name/ID:  ibm/granite-13b-chat-v2 (Or the specific model ID you want to connect to, refer to watsonx model IDs from your IBM watsonx console.)  \\n   * Token Limit: 8,192 (Or the specified model\'s maximum context length. Refer to the watsonx model IDs from your IBM watsonx console.)\\n\\n![Example IBM Granite connection](/img/ibm-granite-connection.png)\\n\\n7. Click **Connect**.\\n8. Click **Name and Connect**.\\n\\n## Step 4\\\\. Create a Configured Model\\n\\nBefore you can use your connected LLM, you need to create a configured model.\\n\\n1. In the Model Library in Einstein Studio, select **Configure Foundation Model**.  \\n2. Select **Create Model**.  \\n3. Choose your connected LLM from the \u201cFoundation Model\u201d dropdown.   \\n4. Configure the model parameters.  \\n5. Test the model performance in the model playground.\\n6. After testing, click **Save** in the bottom right.\\n7. Navigate to the \\"Generative\\" tab of the Einstein Studio Models page and verify that your watsonx configured model appears.\\n8. Select your watsonx model to view the configuration details or create a prompt template.\\n\\nYou can now use your LLM wherever you can use Einstein Studio generative models. For instance, you can build prompt templates using [Prompt Builder](https://www.google.com/url?q=https://help.salesforce.com/s/articleView?id%3Dsf.prompt_builder_about.htm&sa=D&source=docs&ust=1729008016102516&usg=AOvVaw29nKsztGMWSoa59DEIFXZJ). \\n\\n## See Also\\n\\n* [*External:* IBM watsonx Repo](https://ibm.biz/eebl-salesforce-watsonx-apic-connector-code)   \\n* [*Interactive Demo:* Use watsonx AI models from Salesforce](https://dsce.ibm.com/wizard/watsonx/results/watsonx-use-watsonx-ai-models-from-salesforce#)  \\n* [*Salesforce Help:* Add a Foundation Model](https://help.salesforce.com/s/articleView?id=sf.c360_a_ai_foundation_models_create.htm&type=5)"},{"id":"mulesoft","metadata":{"permalink":"/einstein-platform/mulesoft","source":"@site/cookbook/llm-open-connector-mulesoft-mac.mdx","title":"LLM Open Connector + MuleSoft + Cerebras","description":"Learn how to implement Salesforce\'s LLM Open Connector with MuleSoft Anypoint Platform using its AI Chain Connector and Inference Connector.","date":"2024-11-25T00:00:00.000Z","tags":[{"inline":false,"label":"mulesoft","permalink":"/einstein-platform/tags/mulesoft"},{"inline":false,"label":"mac","permalink":"/einstein-platform/tags/mac"},{"inline":false,"label":"llm-open-connector","permalink":"/einstein-platform/tags/llm-open-connector"}],"readingTime":5.345,"hasTruncateMarker":true,"authors":[{"name":"Amir","title":"Developer Advocate @ Salesforce","url":"https://github.com/amirkhan-ak-sf","page":{"permalink":"/einstein-platform/authors/amirkhan-ak-sf"},"socials":{"github":"https://github.com/amirkhan-ak-sf"},"imageURL":"https://github.com/amirkhan-ak-sf.png","key":"amirkhan-ak-sf"}],"frontMatter":{"slug":"mulesoft","authors":["amirkhan-ak-sf"],"tags":["mulesoft","mac","llm-open-connector"],"date":"2024-11-25T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"LLM Open Connector + watsonx","permalink":"/einstein-platform/ibm"},"nextItem":{"title":"LLM Open Connector + MuleSoft + Ollama","permalink":"/einstein-platform/mulesoft-ollama"}},"content":"import ReactPlayer from \'react-player\'\\n\\n\\nLearn how to implement Salesforce\'s [LLM Open Connector](/docs/apis/llm-open-connector/) with MuleSoft Anypoint Platform using its [AI Chain Connector](https://mac-project.ai/docs/mulechain-ai/getting-started) and [Inference Connector](https://mac-project.ai/docs/mac-inference/getting-started). \\n\\n\x3c!-- truncate --\x3e\\nThis recipe implements an example of Cerebras Inference; however, the high-level process is the same for all models and providers.\\n\\n:::note\\nThe steps in this recipe use Anypoint Studio. For instructions using Anypoint Code Builder (ACB), see [LLM Open Connector + MuleSoft + Ollama](.//llm-open-connector-mulesoft-mac-ollama.md).\\n:::\\n\\n## High-level Process\\n\\nThere are four high-level steps for connecting your MuleSoft model endpoint to the LLM Open Connector.\\n\\n![MuleSoft high-level process](../static/img/mule-mac-process.png)\\n\\n## Prerequisites + Tutorial Video\\n\\nBefore you begin, review the prerequisites:\\n\\n1. You have a MuleSoft Account ([Sign up here](https://anypoint.mulesoft.com/login/signup)).\\n2. You have [Anypoint Code Builder (ACB)](https://docs.mulesoft.com/anypoint-code-builder/start-acb-desktop) or [Anypoint Studio](https://www.mulesoft.com/lp/dl/anypoint-mule-studio) installed. The instructions in this recipe are based on Anypoint Studio.\\n3. You have [Inference Connector](https://mac-project.ai/docs/mac-inference/getting-started) installed.\\n4. You have a [Cerebras account](https://inference.cerebras.ai/) with an API key.\\n\\nView a step-by-step tutorial video that covers an implementation similar to the one covered in this recipe:\\n\\n<ReactPlayer playing={false} url=\'https://youtu.be/x4gMffK0Dek?si=Q9_3L__wno3Ca9uZ\'/>\\n\\n## Step 1: Download the API Specification for the LLM Open Connector\\n\\n1. Download LLM Open Connector [API Spec](https://github.com/salesforce/einstein-platform/blob/main/api-specs/llm-open-connector/llm-open-connector.yml).\\n\\n2. Rename the file from `llm-open-connector.yml` to `llm-open-connector.yaml`.\\n\\n## Step 2: Import the API Specification into Anypoint Design Center\\n\\n1. Log in to your MuleSoft account.\\n\\n2. Go to [Anypoint Design Center](https://anypoint.mulesoft.com/designcenter/#/projects). \\n\\n3. Import a new API specification from a file using these values:\\n    - Project Name: `Cerebras-LLM-Provider`,\\n    - API Specification: Select `REST API`,\\n    - File upload: `llm-open-connector.yaml`. Be sure to use the renamed file.\\n\\n4. Click **Import**.\\n\\n5. Verify that the API specification successfully imported. \\n\\n![Import API Specification](../static/img/mule-mac-publish-provider-asset.png)\\n\\n6. Change `termsOfService: \\"\\"` to `termsOfService: \\"-\\"`. \\n\\n7. Remove `servers:` and the example `url`. \\n\\n```\\nservers:\\n  - url: https://bring-your-own-llm.example.com\\n```\\n\\n7. Click **Publish.** \\n\\n8. Provide versioning information:\\n    - Asset version: `1.0.0`\\n    - API version: `v1`\\n    - Lifecycle State: `Stable`\\n\\n![Publish to Exchange](../static/img/mule-mac-publish-to-exchange.png)\\n\\n9. Click **Publish to Exchange**.\\n\\n## Step 3: Implement the API Specification\\n\\nThis cookbook uses Anypoint Studio to implement the API Specification. If you prefer, you can also implement the spec with Anypoint Code Builder.\\n\\n### Import API Specification into Studio\\n\\n1. Open Anypoint Studio and create a Mule Project.\\n\\n2. Name the project and import an API spec:\\n    - Project Name: `cerebras-llm-provider`, \\n    - Import a published API: Select the `Cerebras-LLM-Provider` API spec from the previous step.\\n\\n![Add API to Mule Project](../static/img/mule-mac-scaffold.png)\\n\\n3.  Click **Finish**.\\n\\n### Add the Inference Connector to Your Project\\n\\n1. If you have not installed the Inference Connector, [install it](https://mac-project.ai/docs/mac-inference/getting-started) before you start. \\n2. Add the Inference Connector dependency to the Mule Project.\\n\\n```xml\\n<dependency>\\n  <groupId>com.mulesoft.connectors</groupId>\\n  <artifactId>mac-inference-chain</artifactId>\\n  <version>0.1.0</version>\\n  <classifier>mule-plugin</classifier>\\n</dependency>\\n```\\n3. Make sure the Inference Connector is present in the Mule Palette.\\n![Mule Palette](../static/img/mule-mac-mule-palette-inference.png)\\n\\n\\n### Implement the Chat Completions Endpoint\\n\\n1. Go to the scaffolded flow `post:\\\\chat\\\\completions:application\\\\json:llm-open-connector-config`.\\n\\n2. Drag and drop `Chat completions` operation from the Inference Connector into the Flow.\\n\\n3. Provide the Inference connector configuration for Cerebras.\\n\\n4. Parametrize all properties needed by the LLM Open Connector API Spec.\\n![Configuration Params](../static/img/mule-mac-configuration-params.png).\\n\\n5. In the `Chat completions` operation, enter: `payload.messages`. \\n\\n6. Before the `Chat completions` operation, add the `Set Variable` operation with the name `model` and enter in the expression value `payload.model`.\\n\\n7. After the `Chat completions` operation, add the `Transform Message` operation and provide the mapping in this code block:\\n\\n```json\\n%dw 2.0\\noutput application/json\\n---\\n{\\n\\tid: \\"chatcmpl-\\" ++ now() as Number as String,\\n\\tcreated: now() as Number,\\n\\tusage: {\\n\\t\\tcompletion_tokens: attributes.tokenUsage.outputCount,\\n\\t\\tprompt_tokens: attributes.tokenUsage.inputCount,\\n\\t\\ttotal_tokens: attributes.tokenUsage.totalCount\\n\\t},\\n\\tmodel: vars.model,\\n\\tchoices: [\\n    \\t{\\n      finish_reason: \\"stop\\",\\n      index: 0,\\n      message: {\\n        content: payload.response default \\"\\",\\n        role: \\"assistant\\"\\n      }\\n    }\\n  ],\\n\\tobject: \\"chat.completion\\"\\n}\\n```\\n\\n8. Save the project.\\n\\n\\n### Test Locally\\n\\n1. Start the Mule Application.\\n\\n2. Go to the API Console.\\n![Configuration Params](../static/img/mule-mac-test-locally.png)\\n\\n3. Enter the API Key and following payload:\\n\\n```json\\n{\\n  \\"messages\\": [\\n    {\\n      \\"content\\": \\"What is the capital of Switzerland?\\",\\n      \\"role\\": \\"user\\",\\n      \\"name\\": \\"\\"\\n    }\\n  ],\\n  \\"model\\": \\"llama3.1-8b\\",\\n  \\"max_tokens\\": 500,\\n  \\"n\\": 1,\\n  \\"temperature\\": 0.7,\\n  \\"parameters\\": {\\n    \\"top_p\\": 0.5\\n  }\\n}\\n```\\n\\n4. Validate the result. Make sure the values are mapped correctly for token usage. \\n\\n```json\\n{\\n  \\"id\\": \\"chatcmpl-1732373228\\",\\n  \\"created\\": 1732373228,\\n  \\"usage\\": {\\n    \\"completion_tokens\\": 8,\\n    \\"prompt_tokens\\": 42,\\n    \\"total_tokens\\": 50\\n  },\\n  \\"model\\": \\"llama3.1-8b\\",\\n  \\"choices\\": [\\n    {\\n      \\"finish_reason\\": \\"stop\\",\\n      \\"index\\": 0,\\n      \\"message\\": {\\n        \\"content\\": \\"The capital of Switzerland is Bern.\\",\\n        \\"role\\": \\"assistant\\"\\n      }\\n    }\\n  ],\\n  \\"object\\": \\"chat.completion\\"\\n}\\n```\\n\\n## Step 4. Deploy to Anypoint CloudHub\\n\\n1. After the application is tested successfully, deploy it to Anypoint CloudHub.\\n\\n2. Right-click your Mule Project and navigate to `Anypoint Platform` > `Deploy to CloudHub`.\\n\\n3. Choose the environment you want to deploy to.\\n\\n4. Enter the required values:\\n    - App name: `cerebras-llm-provider`\\n    - Deployment target: `Shared Space (Cloudhub 2.0)`\\n    - Replica Count: `1`\\n    - Replica Size: `Micro (0.1 vCore)`\\n\\n5. Click **Deploy Application**.\\n\\n6. Wait until the application is deployed. \\n\\n![Deployed App](../static/img/mule-mac-deployed-app.png)\\n\\n\\n\\n:::note\\nIf you receive the error `[The asset is invalid, Error while trying to set type: app. Expected type is: rest-api.]`, go to Exchange and delete or rename the asset. [This error is a known issue](https://help.salesforce.com/s/articleView?id=001119384&type=1).\\n:::\\n\\n\\n\\n## Create a Configuration in Model Builder\\n\\nAfter your API is running on CloudHub, you need to add the model endpoint to Model Builder. \\n\\n1. In Salesforce, open Data Cloud. \\n\\n2. Navigate to Einstein Studio.\\n\\n3. Click **Add Foundation Model**, and click **Connect to your LLM**.\\n\\n4. Click **Next**.\\n\\n5. Enter the required values:\\n    - Name: `Cerebras-LLM-Provider`\\n    - URL: `<cloudhub_url>/api`\\n    - Model: A model name is required. For this recipe, choose between `llama3.1-70b` or `llama3.1-8b`.\\n\\n![Model Builder](../static/img/mule-mac-add-model-einstein-studio.png)\\n\\n6. Click **Connect**.\\n\\n7. Create two configurations for each supported model:\\n    - `llama3.1-70b`\\n    - `llama3.1-8b`\\n\\n![Model Builder Playground](../static/img/mule-mac-einstein-playground.png)\\n\\n## Important Considerations\\n\\n1. This cookbook uses Cerebras models `llama3.1-70b` and `llama3.1-8b`.\\n2. When configuring in Model Builder, you need to provide a default value for the model. In this recipe the model name is parametrized, so a value is required. \\n3. The API is deployed under the governance of the MuleSoft Anypoint Platform. As a result: \\n    - You can monitor the application by viewing logs and errors.\\n    - You can apply additional security through Anypoint\'s API management capabilities.\\n    - You can deploy multiple replicas to scale horizontally and vertically.\\n\\n## Conclusion\\nThis cookbook demonstrates how to set up an LLM Open Connector using MuleSoft for the Chat Completion endpoints of Cerebras. This recipe is a sandbox implementation, and it\'s not production ready. For production use, please optimize your implementation based on your specific requirements and expected usage patterns."},{"id":"mulesoft-ollama","metadata":{"permalink":"/einstein-platform/mulesoft-ollama","source":"@site/cookbook/llm-open-connector-mulesoft-mac-ollama.md","title":"LLM Open Connector + MuleSoft + Ollama","description":"Learn how to implement Salesforce\'s LLM Open Connector using MuleSoft and the MuleSoft AI Chain (MAC) project. This recipe relies on locally hosted Ollama for quick testing. The Mule application and MAC project run in CloudHub to expose the API.","date":"2024-11-29T00:00:00.000Z","tags":[{"inline":false,"label":"mulesoft","permalink":"/einstein-platform/tags/mulesoft"},{"inline":false,"label":"mac","permalink":"/einstein-platform/tags/mac"},{"inline":false,"label":"llm-open-connector","permalink":"/einstein-platform/tags/llm-open-connector"},{"inline":false,"label":"ollama","permalink":"/einstein-platform/tags/ollama"}],"readingTime":6.81,"hasTruncateMarker":true,"authors":[{"name":"Alex Martinez","title":"Developer Advocate @ Salesforce","url":"https://github.com/alexandramartinez","page":{"permalink":"/einstein-platform/authors/alexandramartinez"},"socials":{"github":"https://github.com/alexandramartinez","linkedin":"https://www.linkedin.com/in/alexandra-n-martinez/"},"imageURL":"https://github.com/alexandramartinez.png","key":"alexandramartinez"}],"frontMatter":{"slug":"mulesoft-ollama","authors":["alexandramartinez"],"tags":["mulesoft","mac","llm-open-connector","ollama"],"date":"2024-11-29T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"LLM Open Connector + MuleSoft + Cerebras","permalink":"/einstein-platform/mulesoft"}},"content":"Learn how to implement Salesforce\'s [LLM Open Connector](/docs/apis/llm-open-connector/) using MuleSoft and the [MuleSoft AI Chain (MAC)](https://mac-project.ai/) project. This recipe relies on locally hosted [Ollama](https://ollama.com/) for quick testing. The Mule application and MAC project run in CloudHub to expose the API.\\n\\n\x3c!-- truncate --\x3e\\n:::note\\nThe steps in this recipe use Anypoint Code Builder (ACB). For instructions using Anypoint Studio, see [LLM Open Connector + MuleSoft + Cerebras](../cookbook/llm-open-connector-mulesoft-mac.mdx).\\n:::\\n\\n## Prerequisites\\n\\n1. Anypoint Platform account (create a free trial [here](https://anypoint.mulesoft.com/login/signup)).\\n2. MuleSoft\'s IDE: either [Anypoint Code Builder (ACB)](https://docs.mulesoft.com/anypoint-code-builder/start-acb-desktop) or [Anypoint Studio](https://www.mulesoft.com/lp/dl/anypoint-mule-studio). This recipe is based on ACB.\\n3. [Ollama](https://ollama.com/) locally running the model of your choice (for example, `llama3`).\\n4. [Ngrok](https://download.ngrok.com/) to expose your local Ollama instance to the public cloud.\\n5. A REST client application to test the API calls. For example, cURL or Postman.\\n\\nFor troubleshooting, you can compare your Mule app with [this GitHub repo](https://github.com/alexandramartinez/ollama-llm-provider).\\n\\n## Expose the Ollama URL\\n\\n1. Verify your model is up and running (locally) in Ollama using `ollama list` or `ollama ps`.\\n\\n2. Run this command to start ngrok:\\n\\n    ```shell\\n    ngrok http 11434 --host-header=\\"localhost:11434\\"\\n    ```\\n\\n3. Copy the address from the **Forwarding** field. This address is the URL you need to connect to Ollama from the Mule app.\\n\\n## Create and Publish the API Specification\\n\\n1. Download the `llm-open-connector.yml` file from the [einstein-platform](https://github.com/salesforce/einstein-platform/blob/main/api-specs/llm-open-connector/llm-open-connector.yml) GitHub repository.\\n\\n2. Rename the extension of the file from `yml` to `yaml`.\\n\\n3. Log in to your [Anypoint Platform](https://anypoint.mulesoft.com) account.\\n\\n4. Navigate to **Design Center**.\\n\\n5. Click **Create +** > **Import from File**.\\n\\n6. Fill out the spec details:\\n\\n    - Project name: `LLM Open Connector`\\n    - Import type: API specification `REST API`\\n    - Import file: The `llm-open-connector.yaml` file from steps 1 and 2. (Ensure that the file extension is `.yaml`.)\\n\\n7. Click **Import**.\\n\\n8. Add a character inside `termsOfService` (for example, `termsOfService: \\"-\\"`).\\n\\n9. Remove the `servers` section, including the example `url`:\\n\\n    ```yaml\\n    servers:\\n        - url: https://bring-your-own-llm.example.com\\n    ```\\n\\n10. Click **Publish** in the top right of the screen.\\n\\n11. Provide versioning information:\\n\\n    - Asset version: `1.0.0`\\n    - API version: `v1`\\n    - LifeCycle State: Stable\\n\\n12. Click **Publish to Exchange**.\\n\\n13. After it is published, you can click the **Exchange** link to see the preview of the published asset, or click **Close**.\\n\\n14. Exit Design Center.\\n\\n## Implement the Mule App\\n\\n1. Head to the IDE of your choice. In this recipe, we are using Anypoint Code Builder, the MuleSoft IDE powered by Visual Studio Code.\\n\\n2. In ACB, click **Implement an API**.\\n\\n    :::note\\n    If you haven\'t signed in to your Anypoint Platform account through ACB, it asks you to sign in. Follow the prompts to sign in.\\n    :::\\n\\n3. Fill out the project details:\\n\\n    - Project Name: `ollama-llm-provider`\\n    - Project Location: choose any folder to keep this project\\n    - Search an API Specification from Exchange to implement it: the **LLM Open Connector** we just published\\n    - Mule Runtime: `4.8.0`\\n    - Java Version: `17`\\n\\n4. Click **Create Project** and wait for it to be fully processed.\\n\\n### Maven\\n\\n1. After it is processed, open the `pom.xml` file and add the Maven dependency:\\n\\n    ```xml\\n    <dependency>\\n        <groupId>com.mulesoft.connectors</groupId>\\n        <artifactId>mule4-aichain-connector</artifactId>\\n        <version>1.0.0</version>\\n        <classifier>mule-plugin</classifier>\\n    </dependency>\\n    ```\\n\\n2. Change the `version` in line 6:\\n    \\n    ```xml\\n    <version>1.0.0</version>\\n    ```\\n \\n3. Copy the `groupId` (in `dependencies`) for the `llm-open-connector` artifact. The `groupId` is a number similar to: `d62b8a81-f143-4534-bb89-3673ad61ah01`. This number is your organization ID.\\n\\n4. Paste this organization ID in the `groupId` field at the top of the file, replacing `com.mycompany`:\\n\\n    ```xml\\n    <groupId>d62b8a81-f143-4534-bb89-3673ad61ah01</groupId>\\n    ```\\n\\n### LLM Config\\n\\n1. Create a file under src/main/resources named `llm-config.json`.\\n\\n2. Paste this code into the `llm-config.json` file:\\n\\n    ```json\\n    {\\n        \\"OLLAMA\\": {\\n            \\"OLLAMA_BASE_URL\\": \\"https://11-13-23-16-11.ngrok-free.app\\"\\n        }\\n    }\\n    ```\\n\\n3. Replace the example URL with the one you copied from ngrok in step 3 of [Expose the Ollama URL](#expose-the-ollama-url).\\n\\n### Mule Flow\\n\\n1. Open the `ollama-llm-provider.xml` file under src/main/mule.\\n\\n2. Add this code under `apikit:config` and before `<flow name=\\"llm-open-connector-main\\">`.\\n\\n    ```xml\\n    <ms-aichain:config configType=\\"Configuration Json\\" filePath=\'#[(mule.home default \\"\\") ++ \\"/apps/\\" ++ (app.name default \\"\\") ++ \\"/llm-config.json\\"]\' llmType=\\"OLLAMA\\" modelName=\\"#[vars.inputPayload.model default \'llama3\']\\" name=\\"MAC_Config\\" temperature=\\"#[vars.inputPayload.temperature default 1]\\" maxTokens=\\"#[vars.inputPayload.max_tokens default 500]\\"></ms-aichain:config>\\n    ```\\n\\n    :::note\\n    If your model is not `llama3`, make sure you upload the `default` value in the previous line (for example, `modelName=\\"#[vars.inputPayload.model default \'llama3\']\\"`).\\n    :::\\n\\n3. Locate the last flow in the file (for example, `post:\\\\chat\\\\completions:application\\\\json:llm-open-connector-config`).\\n\\n4. Remove the logger and add this code to the flow.\\n\\n    ```xml\\n    <logger doc:name=\\"Logger\\" doc:id=\\"ezzhif\\" message=\\"#[payload]\\" />\\n    <ee:transform doc:name=\\"Transform\\" doc:id=\\"dstcls\\">\\n        <ee:message>\\n            <ee:set-payload><![CDATA[(payload.messages filter ($.role == \\"user\\"))[0].content default payload.messages[0].content]]></ee:set-payload>\\n        </ee:message>\\n        <ee:variables>\\n            <ee:set-variable variableName=\\"inputPayload\\">\\n                <![CDATA[payload]]>\\n            </ee:set-variable>\\n        </ee:variables>\\n    </ee:transform>\\n    <ms-aichain:chat-answer-prompt config-ref=\\"MAC_Config\\" doc:id=\\"mmoptd\\" doc:name=\\"Chat answer prompt\\" />\\n    <ee:transform doc:name=\\"Transform\\" doc:id=\\"czdqgi\\">\\n        <ee:message>\\n            <ee:set-payload>\\n                    <![CDATA[output application/json\\n    ---\\n    {\\n        id: uuid( ),\\n        choices: [\\n            {\\n                finish_reason: \\"stop\\",\\n                index: 0,\\n                message: {\\n                    content: payload.response default \\"\\",\\n                    role: \\"assistant\\"\\n                }\\n            }\\n        ],\\n        created: now() as Number,\\n        model: vars.inputPayload.model default \\"\\",\\n        object: \\"chat.completion\\",\\n        usage: {\\n            completion_tokens: (attributes.tokenUsage.outputCount default 0) as Number,\\n            prompt_tokens: (attributes.tokenUsage.inputCount default 0) as Number,\\n            total_tokens: (attributes.tokenUsage.totalCount default 0) as Number\\n        }\\n    }]]>\\n            </ee:set-payload>\\n        </ee:message>\\n    </ee:transform>\\n    <logger doc:name=\\"Logger\\" doc:id=\\"rzhuiw\\" message=\\"#[payload]\\" />\\n    ```\\n\\n5. Save all the files.\\n\\n## Deploy to CloudHub 2.0\\n\\n:::note\\nTo verify your setup, you can test your application locally before deploying to CloudHub. Navigate to the **Run and Debug** tab, click the green **run** button, and follow [Test the Deployed Mule App](#test-the-deployed-mule-app). Be sure to replace the URL with `http://localhost:8081/api/chat/completions`.\\n:::\\n\\n1. With the `ollama-llm-provider.xml` file open, click **Deploy to CloudHub** in the top right of the screen (the rocket \ud83d\ude80 icon).\\n\\n2. Select **CloudHub 2.0**.\\n\\n3. Select **Cloudhub-US-East-2** if you have a free trial account based on the US cloud. Otherwise, select any of the regions available to you.\\n\\n    :::note\\n    To verify the regions you have access to in your Anypoint Platform account, you can do so from the **Runtime Manager** by creating a deployment and checking the available options.\\n    :::\\n\\n4. Select **Sandbox** as the environment.\\n\\n5. A new file named `deploy_ch2.json` is created under src/main/resources. You can change the data in this file if needed. By default, it contains:\\n\\n    ```json\\n    {\\n        \\"applicationName\\": \\"ollama-llm-provider\\",\\n        \\"runtime\\": \\"4.8.0\\",\\n        \\"replicas\\": 1,\\n        \\"replicaSize\\": \\"0.1\\",\\n        \\"deploymentModel\\": \\"rolling\\"\\n    }\\n    ```\\n\\n6. In the message that appears in the bottom right, click **Deploy**.\\n\\n7. In the next prompt, select the latest Mule runtime version available. For this recipe, you can select `4.8.1:6e-java17` if available. This action changes the `runtime` field in the `deploy_ch2.json` file.\\n\\n8. Your asset is first published to Exchange as an Application. Afterwards, the deployment to CloudHub 2.0 starts. When you receive the message: *\'ollama-llm-provider\' is deployed to \'Sandbox\' Env in CloudHub 2.0*, the deployment has successfully started in Anypoint Platform.\\n\\n9. Navigate to your Anypoint Platform account and open **Runtime Manager** (or click **Open in Runtime Manager** in ACB).\\n\\n10. When your application\'s status appears as \ud83d\udfe2 `Running`, it\'s ready to start receiving requests. Copy the public endpoint URL (for example, `https://ollama-llm-provider-69s5jr.5se6i9-2.usa-e2.cloudhub.io`).\\n\\n11. Add `/api` at the end of the URL. This URL is used to call the API.\\n\\n## Test the Deployed Mule App\\n\\nYou can call the previous URL using tools like cURL or Postman. Here is an example cURL command; make sure you replace the example URL with your own.\\n\\n```shell\\ncurl -X POST https://ollama-llm-provider-69s5jr.5se6i9-2.usa-e2.cloudhub.io/api/chat/completions \\\\\\n-H \'Content-Type: application/json\' \\\\\\n-d \'{\\n  \\"messages\\": [\\n    {\\n      \\"content\\": \\"What is the capital of Canada?\\",\\n      \\"role\\": \\"user\\",\\n      \\"name\\": \\"Alex\\"\\n    }\\n  ],\\n  \\"model\\": \\"llama3\\",\\n  \\"max_tokens\\": 0,\\n  \\"n\\": 1,\\n  \\"temperature\\": 1,\\n  \\"parameters\\": {\\n    \\"top_p\\": 0.5\\n  }\\n}\'\\n```\\n\\n:::note\\nSince Ollama does not require an `api-key` header, we are not sending it in this request.\\n:::\\n\\n## Bring Your Connected Endpoint to Salesforce Model Builder\\n\\nFollow the instructions in [this developer blog](https://developer.salesforce.com/blogs/2024/10/build-generative-ai-solutions-with-llm-open-connector) to use your model in Model Builder. When you activate your model, you can use it within [Prompt Builder](https://developer.salesforce.com/docs/einstein/genai/guide/get-started-prompt-builder.html), the [Models API](https://developer.salesforce.com/docs/einstein/genai/guide/models-api.html), and for building actions using prompt templates in Agent Builder. All these methods provide built-in security offered by the [Einstein Trust Layer](https://help.salesforce.com/s/articleView?id=sf.generative_ai_trust_layer.htm&type=5).\\n\\n## Conclusion\\n\\nThis cookbook demonstrates how to set up the LLM Open Connector using Ollama locally for Chat Completion endpoints for various models. This option is recommended for local testing since there are no credit limits. Another scenario is if you want to test your own model locally using Ollama. The implementation in this recipe is not recommended for production use unless you plan to host your model on-premises."}]}}')}}]);